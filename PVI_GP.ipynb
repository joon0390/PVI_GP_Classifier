{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d6492b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from torch.distributions import MultivariateNormal, kl_divergence, Normal\n",
    "\n",
    "# ==========================================\n",
    "# 1. PVI-GP Classifier Model\n",
    "# ==========================================\n",
    "class PVIGPClassifier(nn.Module):\n",
    "    def __init__(self, X_train):\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.n = X_train.shape[0]\n",
    "        self.jitter = 1e-5\n",
    "        \n",
    "        # 1. Hyperparameters (Kernel)\n",
    "        # Softplus를 통과시킬 것이므로 초기값은 적당히 설정\n",
    "        self.log_lengthscale = nn.Parameter(torch.tensor(np.log(0.5)))\n",
    "        self.log_scale = nn.Parameter(torch.tensor(np.log(1.0)))\n",
    "        \n",
    "        # 2. Variational Parameters q(f)\n",
    "        # q(f) ~ N(mu, L*L^T)\n",
    "        self.q_mu = nn.Parameter(torch.zeros(self.n))\n",
    "        self.q_L_vec = nn.Parameter(torch.randn(self.n * (self.n + 1) // 2) * 0.05)\n",
    "\n",
    "    def kernel_matrix(self, x1, x2):\n",
    "        # [Robust] Lengthscale이 0으로 수렴하는 것을 방지 (Softplus + 0.1)\n",
    "        lengthscale = F.softplus(self.log_lengthscale) + 0.1\n",
    "        scale = F.softplus(self.log_scale) + 0.1\n",
    "        \n",
    "        dist_sq = torch.cdist(x1, x2, p=2) ** 2\n",
    "        return scale * torch.exp(-0.5 * dist_sq / (lengthscale ** 2))\n",
    "\n",
    "    def get_q_dist(self):\n",
    "        # Construct Covariance Matrix from Cholesky factor L\n",
    "        L = torch.zeros(self.n, self.n, device=self.X_train.device)\n",
    "        indices = torch.tril_indices(row=self.n, col=self.n)\n",
    "        L[indices[0], indices[1]] = self.q_L_vec\n",
    "        \n",
    "        # 대각 성분은 양수여야 함 (Exp 처리)\n",
    "        diag_idx = range(self.n)\n",
    "        L[diag_idx, diag_idx] = torch.exp(L[diag_idx, diag_idx])\n",
    "        \n",
    "        q_cov = L @ L.t() + torch.eye(self.n, device=self.X_train.device) * self.jitter\n",
    "        return MultivariateNormal(self.q_mu, covariance_matrix=q_cov)\n",
    "\n",
    "    def get_p_dist(self):\n",
    "        # Prior p(f) ~ N(0, K)\n",
    "        K = self.kernel_matrix(self.X_train, self.X_train)\n",
    "        K = K + torch.eye(self.n, device=self.X_train.device) * self.jitter\n",
    "        return MultivariateNormal(torch.zeros(self.n, device=self.X_train.device), covariance_matrix=K)\n",
    "\n",
    "    def predict(self, X_test, num_samples=1000):\n",
    "        # [Efficient] O(N^3) 회피 및 대각 성분(Marginal Variance)만 계산\n",
    "        K_xx = self.kernel_matrix(self.X_train, self.X_train) + torch.eye(self.n, device=self.X_train.device) * self.jitter\n",
    "        K_sx = self.kernel_matrix(X_test, self.X_train)\n",
    "        K_ss_diag = self.kernel_matrix(X_test, X_test).diag()\n",
    "        \n",
    "        q_dist = self.get_q_dist()\n",
    "        \n",
    "        # Linear Algebra for Mean/Var\n",
    "        K_inv_mu = torch.linalg.solve(K_xx, q_dist.mean)\n",
    "        K_inv_K_xs_T = torch.linalg.solve(K_xx, K_sx.t())\n",
    "        \n",
    "        mu_star = K_sx @ K_inv_mu\n",
    "        \n",
    "        # 대각 성분만 효율적으로 계산 (sum(dim=1))\n",
    "        term2 = (K_sx * K_inv_K_xs_T.t()).sum(dim=1)\n",
    "        B = torch.linalg.solve(K_xx, q_dist.covariance_matrix @ K_inv_K_xs_T)\n",
    "        term3 = (K_sx * B.t()).sum(dim=1)\n",
    "        \n",
    "        var_star = (K_ss_diag - term2 + term3).clamp(min=1e-6)\n",
    "        \n",
    "        # Sampling for Prediction\n",
    "        q_star = Normal(mu_star, torch.sqrt(var_star))\n",
    "        f_samples = q_star.sample((num_samples,))\n",
    "        probs = torch.sigmoid(f_samples).mean(dim=0)\n",
    "        \n",
    "        return probs\n",
    "\n",
    "# ==========================================\n",
    "# [cite_start]2. PVI Objective (Log Score) [cite: 81, 88]\n",
    "# ==========================================\n",
    "def pvi_objective(model, y_target, num_samples=32, lambda_reg=0.05):\n",
    "    q_dist = model.get_q_dist()\n",
    "    \n",
    "    # 1. Reparameterization Trick\n",
    "    f_samples = q_dist.rsample((num_samples,)) \n",
    "    \n",
    "    # 2. Logistic Link\n",
    "    probs = torch.sigmoid(f_samples)\n",
    "    likelihoods = torch.where(y_target.unsqueeze(0) == 1, probs, 1 - probs)\n",
    "    \n",
    "    # 3. [Key] Log-Mean-Exp (PVI)\n",
    "    # Mean(Integral) first, then Log.\n",
    "    predictive_likelihood = likelihoods.mean(dim=0) \n",
    "    log_score = torch.log(predictive_likelihood + 1e-10).sum()\n",
    "    \n",
    "    # 4. Regularization\n",
    "    kl_reg = kl_divergence(q_dist, model.get_p_dist())\n",
    "    \n",
    "    return -log_score + lambda_reg * kl_reg\n",
    "\n",
    "# ==========================================\n",
    "# 3. Training Loop\n",
    "# ==========================================\n",
    "def train():\n",
    "    # Data: Noisy Moons (to test robustness)\n",
    "    X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    model = PVIGPClassifier(X)\n",
    "    \n",
    "    # Differential Learning Rates\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': [model.q_mu, model.q_L_vec], 'lr': 0.02}, # q(f)는 빠르게\n",
    "        {'params': [model.log_lengthscale, model.log_scale], 'lr': 0.01} # Kernel은 천천히\n",
    "    ])\n",
    "    \n",
    "    print(\"Training PVI-GP (Logistic Likelihood)...\")\n",
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad()\n",
    "        loss = pvi_objective(model, y, num_samples=32, lambda_reg=0.05)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoch {epoch:4d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model, X, y\n",
    "\n",
    "# ==========================================\n",
    "# 4. Visualization\n",
    "# ==========================================\n",
    "def visualize(model, X_train, y_train):\n",
    "    model.eval()\n",
    "    \n",
    "    x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "    \n",
    "    X_test = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs = model.predict(X_test, num_samples=500)\n",
    "        probs = probs.reshape(xx.shape)\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    contour = plt.contourf(xx, yy, probs, levels=20, cmap='RdBu', alpha=0.8, vmin=0, vmax=1)\n",
    "    plt.colorbar(contour, label=\"Predictive Probability\")\n",
    "    \n",
    "    plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='blue', edgecolors='k', label='Class 0')\n",
    "    plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='red', edgecolors='k', label='Class 1')\n",
    "    \n",
    "    plt.title(\"PVI GP Classifier (Logistic Link, No Approximation)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model, X, y = train()\n",
    "    visualize(trained_model, X, y)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
